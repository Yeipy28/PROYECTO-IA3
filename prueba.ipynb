{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from sklearn.metrics import f1_score\n","\n","def calculate_f1_macro(model, X_val, y_val):\n","    y_pred = model.predict(X_val).argmax(axis=1)\n","    y_true = y_val.argmax(axis=1)\n","    return f1_score(y_true, y_pred, average='macro')\n","f1_A = calculate_f1_macro(model_A, X_A_val, y_A_val)\n","f1_B = calculate_f1_macro(model_B, X_B_val, y_B_val)\n","f1_C = calculate_f1_macro(model_C, X_C_val, y_C_val)\n","\n","print(\"F1 Macro Modelo A:\", f1_A)\n","print(\"F1 Macro Modelo B:\", f1_B)\n","print(\"F1 Macro Modelo C:\", f1_C)\n","from sklearn.metrics import classification_report\n","\n","print(\"\\nüìå Classification Report ‚Äî Modelo A\")\n","print(classification_report(y_A_val.argmax(axis=1),\n","                            model_A.predict(X_A_val).argmax(axis=1)))\n","\n","print(\"\\nüìå Classification Report ‚Äî Modelo B\")\n","print(classification_report(y_B_val.argmax(axis=1),\n","                            model_B.predict(X_B_val).argmax(axis=1)))\n","\n","print(\"\\nüìå Classification Report ‚Äî Modelo C\")\n","print(classification_report(y_C_val.argmax(axis=1),\n","                            model_C.predict(X_C_val).argmax(axis=1)))\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def plot_confusion(model, X_val, y_val, title):\n","    y_pred = model.predict(X_val).argmax(axis=1)\n","    y_true = y_val.argmax(axis=1)\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    plt.figure(figsize=(6,4))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.title(title)\n","    plt.xlabel(\"Predicci√≥n\")\n","    plt.ylabel(\"Real\")\n","    plt.show()\n","\n","plot_confusion(model_A, X_A_val, y_A_val, \"Matriz de Confusi√≥n - Modelo A\")\n","plot_confusion(model_B, X_B_val, y_B_val, \"Matriz de Confusi√≥n - Modelo B\")\n","plot_confusion(model_C, X_C_val, y_C_val, \"Matriz de Confusi√≥n - Modelo C\")\n","# Crear tabla comparativa final\n","df_eval = pd.DataFrame({\n","    \"Modelo\": [\"A_class_weight\", \"B_oversampling\", \"C_over+weight\"],\n","    \"Accuracy\": [\n","        results[\"model_A\"][\"accuracy\"],\n","        results[\"model_B\"][\"accuracy\"],\n","        results[\"model_C\"][\"accuracy\"]\n","    ],\n","    \"Loss\": [\n","        results[\"model_A\"][\"loss\"],\n","        results[\"model_B\"][\"loss\"],\n","        results[\"model_C\"][\"loss\"]\n","    ],\n","    \"Tiempo (s)\": [\n","        results[\"model_A\"][\"time\"],\n","        results[\"model_B\"][\"time\"],\n","        results[\"model_C\"][\"time\"]\n","    ],\n","    \"F1 Macro\": [f1_A, f1_B, f1_C]\n","})\n","\n","print(\"\\nüìä TABLA COMPARATIVA FINAL:\")\n","display(df_eval)\n","\n","# Seleccionar el mejor modelo basado en F1 Macro\n","best_index = df_eval[\"F1 Macro\"].idxmax()\n","best_model_name = df_eval.loc[best_index, \"Modelo\"]\n","best_f1 = df_eval.loc[best_index, \"F1 Macro\"]\n","\n","print(f\"\\nüèÜ Mejor modelo seg√∫n F1 Macro: {best_model_name}\")\n","print(f\"üîç F1 Macro obtenido: {best_f1:.4f}\")\n","\n","# Asignaci√≥n del modelo seleccionado\n","if best_model_name == \"A_class_weight\":\n","    best_model = model_A\n","    X_val_best = X_A_val\n","    y_val_best = y_A_val\n","elif best_model_name == \"B_oversampling\":\n","    best_model = model_B\n","    X_val_best = X_B_val\n","    y_val_best = y_B_val\n","else:\n","    best_model = model_C\n","    X_val_best = X_C_val\n","    y_val_best = y_C_val\n","\n","print(\"\\nüìå Modelo final listo para inferencias como 'best_model'\")\n","\n","# Diccionario de emociones\n","emotion_dict = {\n","    0: \"sadness\",\n","    1: \"joy\",\n","    2: \"love\",\n","    3: \"anger\",\n","    4: \"fear\",\n","    5: \"surprise\"\n","}\n","\n","# Funci√≥n para limpiar texto (igual que en el dataset)\n","def clean_text_input(text):\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n","    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n","    text = re.sub(r\"[^A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±\\s]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","# Preparar texto para el modelo\n","def prepare_input(sentence):\n","    cleaned = clean_text_input(sentence)\n","    seq = tokenizer.texts_to_sequences([cleaned])\n","    pad_seq = pad_sequences(seq, maxlen=max_len, padding='post')\n","    return pad_seq\n","\n","# Funci√≥n principal para predecir emoci√≥n\n","def predict_emotion(sentence, model):\n","    x_input = prepare_input(sentence)\n","    pred = model.predict(x_input)[0]\n","\n","    emotion_id = pred.argmax()\n","    emotion_label = emotion_dict[emotion_id]\n","\n","    print(f\"\\nüìù Texto ingresado:\\n{sentence}\")\n","    print(f\"\\nüéØ Emoci√≥n predicha: **{emotion_label.upper()}**\\n\")\n","\n","    print(\"üìä Probabilidades por emoci√≥n:\")\n","    for idx, prob in enumerate(pred):\n","        print(f\"  {emotion_dict[idx]:10s}: {prob:.4f}\")\n","\n","    return emotion_label, pred\n","\n","predict_emotion(\"I feel extremely happy today!\", best_model)\n","predict_emotion(\"I'm very scared about the future...\", best_model)\n","predict_emotion(\"I love the way you talk to me.\", best_model)\n","predict_emotion(\"This makes me so angry!\", best_model)\n","predict_emotion(\"I can't believe this happened!\", best_model)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Funci√≥n para obtener la atenci√≥n del modelo\n","def get_attention_weights(sentence, model):\n","    x_input = prepare_input(sentence)\n","\n","    # Ejecutar modelo y extraer atenci√≥n\n","    # Atenci√≥n est√° en la capa 'attention' del modelo\n","    attention_layer = model.get_layer('attention')\n","    attention_model = tf.keras.Model(inputs=model.input, outputs=attention_layer.output)\n","\n","    attention_scores = attention_model.predict(x_input)[0]  # vector tama√±o max_len\n","    return attention_scores\n","\n","# Visualizar atenci√≥n como mapa de calor\n","def plot_attention(sentence, model):\n","    cleaned = clean_text_input(sentence)\n","    words = cleaned.split()\n","\n","    attn = get_attention_weights(sentence, model)\n","    attn = attn[:len(words)]  # cortar al n√∫mero de palabras reales\n","\n","    plt.figure(figsize=(12, 1.5))\n","    plt.imshow([attn], cmap='viridis', aspect='auto')\n","    plt.colorbar()\n","    plt.xticks(ticks=np.arange(len(words)), labels=words, rotation=45)\n","    plt.yticks([])\n","    plt.title(\"Mapa de Atenci√≥n ‚Äî Importancia por palabra\")\n","    plt.show()\n","\n","sentence = \"I feel very sad and lonely today.\"\n","plot_attention(sentence, best_model)\n","predict_emotion(sentence, best_model)\n"],"metadata":{"id":"5bJGOJwKTtdA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f1_A = calculate_f1_macro(model_A, X_A_val, y_A_val)\n","f1_B = calculate_f1_macro(model_B, X_B_val, y_B_val)\n","f1_C = calculate_f1_macro(model_C, X_C_val, y_C_val)\n","\n","print(\"F1 Macro Modelo A:\", f1_A)\n","print(\"F1 Macro Modelo B:\", f1_B)\n","print(\"F1 Macro Modelo C:\", f1_C)\n"],"metadata":{"id":"lMPECcmxUL0o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","print(\"\\nüìå Classification Report ‚Äî Modelo A\")\n","print(classification_report(y_A_val.argmax(axis=1),\n","                            model_A.predict(X_A_val).argmax(axis=1)))\n","\n","print(\"\\nüìå Classification Report ‚Äî Modelo B\")\n","print(classification_report(y_B_val.argmax(axis=1),\n","                            model_B.predict(X_B_val).argmax(axis=1)))\n","\n","print(\"\\nüìå Classification Report ‚Äî Modelo C\")\n","print(classification_report(y_C_val.argmax(axis=1),\n","                            model_C.predict(X_C_val).argmax(axis=1)))\n"],"metadata":{"id":"-y0P90uHUOL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def plot_confusion(model, X_val, y_val, title):\n","    y_pred = model.predict(X_val).argmax(axis=1)\n","    y_true = y_val.argmax(axis=1)\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    plt.figure(figsize=(6,4))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.title(title)\n","    plt.xlabel(\"Predicci√≥n\")\n","    plt.ylabel(\"Real\")\n","    plt.show()\n","\n","plot_confusion(model_A, X_A_val, y_A_val, \"Matriz de Confusi√≥n - Modelo A\")\n","plot_confusion(model_B, X_B_val, y_B_val, \"Matriz de Confusi√≥n - Modelo B\")\n","plot_confusion(model_C, X_C_val, y_C_val, \"Matriz de Confusi√≥n - Modelo C\")\n"],"metadata":{"id":"IktBIUv5UPZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear tabla comparativa final\n","df_eval = pd.DataFrame({\n","    \"Modelo\": [\"A_class_weight\", \"B_oversampling\", \"C_over+weight\"],\n","    \"Accuracy\": [\n","        results[\"model_A\"][\"accuracy\"],\n","        results[\"model_B\"][\"accuracy\"],\n","        results[\"model_C\"][\"accuracy\"]\n","    ],\n","    \"Loss\": [\n","        results[\"model_A\"][\"loss\"],\n","        results[\"model_B\"][\"loss\"],\n","        results[\"model_C\"][\"loss\"]\n","    ],\n","    \"Tiempo (s)\": [\n","        results[\"model_A\"][\"time\"],\n","        results[\"model_B\"][\"time\"],\n","        results[\"model_C\"][\"time\"]\n","    ],\n","    \"F1 Macro\": [f1_A, f1_B, f1_C]\n","})\n","\n","print(\"\\nüìä TABLA COMPARATIVA FINAL:\")\n","display(df_eval)\n"],"metadata":{"id":"haDYPDNcUQ22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Seleccionar el mejor modelo basado en F1 Macro\n","best_index = df_eval[\"F1 Macro\"].idxmax()\n","best_model_name = df_eval.loc[best_index, \"Modelo\"]\n","best_f1 = df_eval.loc[best_index, \"F1 Macro\"]\n","\n","print(f\"\\nüèÜ Mejor modelo seg√∫n F1 Macro: {best_model_name}\")\n","print(f\"üîç F1 Macro obtenido: {best_f1:.4f}\")\n","\n","# Asignaci√≥n del modelo seleccionado\n","if best_model_name == \"A_class_weight\":\n","    best_model = model_A\n","    X_val_best = X_A_val\n","    y_val_best = y_A_val\n","elif best_model_name == \"B_oversampling\":\n","    best_model = model_B\n","    X_val_best = X_B_val\n","    y_val_best = y_B_val\n","else:\n","    best_model = model_C\n","    X_val_best = X_C_val\n","    y_val_best = y_C_val\n","\n","print(\"\\nüìå Modelo final listo para inferencias como 'best_model'\")\n"],"metadata":{"id":"EPPRYcHkUSTk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Diccionario de emociones\n","emotion_dict = {\n","    0: \"sadness\",\n","    1: \"joy\",\n","    2: \"love\",\n","    3: \"anger\",\n","    4: \"fear\",\n","    5: \"surprise\"\n","}\n","\n","# Funci√≥n para limpiar texto (igual que en el dataset)\n","def clean_text_input(text):\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n","    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n","    text = re.sub(r\"[^A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±\\s]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","# Preparar texto para el modelo\n","def prepare_input(sentence):\n","    cleaned = clean_text_input(sentence)\n","    seq = tokenizer.texts_to_sequences([cleaned])\n","    pad_seq = pad_sequences(seq, maxlen=max_len, padding='post')\n","    return pad_seq\n","\n","# Funci√≥n principal para predecir emoci√≥n\n","def predict_emotion(sentence, model):\n","    x_input = prepare_input(sentence)\n","    pred = model.predict(x_input)[0]\n","\n","    emotion_id = pred.argmax()\n","    emotion_label = emotion_dict[emotion_id]\n","\n","    print(f\"\\nüìù Texto ingresado:\\n{sentence}\")\n","    print(f\"\\nüéØ Emoci√≥n predicha: **{emotion_label.upper()}**\\n\")\n","\n","    print(\"üìä Probabilidades por emoci√≥n:\")\n","    for idx, prob in enumerate(pred):\n","        print(f\"  {emotion_dict[idx]:10s}: {prob:.4f}\")\n","\n","    return emotion_label, pred\n"],"metadata":{"id":"HYAaCWrgUTi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_emotion(\"I feel extremely happy today!\", best_model)\n","predict_emotion(\"I'm very scared about the future...\", best_model)\n","predict_emotion(\"I love the way you talk to me.\", best_model)\n","predict_emotion(\"This makes me so angry!\", best_model)\n","predict_emotion(\"I can't believe this happened!\", best_model)\n"],"metadata":{"id":"HhtPXe3zUUvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Funci√≥n para obtener la atenci√≥n del modelo\n","def get_attention_weights(sentence, model):\n","    x_input = prepare_input(sentence)\n","\n","    # Ejecutar modelo y extraer atenci√≥n\n","    # Atenci√≥n est√° en la capa 'attention' del modelo\n","    attention_layer = model.get_layer('attention')\n","    attention_model = tf.keras.Model(inputs=model.input, outputs=attention_layer.output)\n","\n","    attention_scores = attention_model.predict(x_input)[0]  # vector tama√±o max_len\n","    return attention_scores\n","\n","# Visualizar atenci√≥n como mapa de calor\n","def plot_attention(sentence, model):\n","    cleaned = clean_text_input(sentence)\n","    words = cleaned.split()\n","\n","    attn = get_attention_weights(sentence, model)\n","    attn = attn[:len(words)]  # cortar al n√∫mero de palabras reales\n","\n","    plt.figure(figsize=(12, 1.5))\n","    plt.imshow([attn], cmap='viridis', aspect='auto')\n","    plt.colorbar()\n","    plt.xticks(ticks=np.arange(len(words)), labels=words, rotation=45)\n","    plt.yticks([])\n","    plt.title(\"Mapa de Atenci√≥n ‚Äî Importancia por palabra\")\n","    plt.show()\n"],"metadata":{"id":"JcrOsH4tUWUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = \"I feel very sad and lonely today.\"\n","plot_attention(sentence, best_model)\n","predict_emotion(sentence, best_model)\n"],"metadata":{"id":"gJjMEHASUX6R"},"execution_count":null,"outputs":[]}]}